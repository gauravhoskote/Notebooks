{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simplenn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYZW9xXGhQfUCLolFUev7p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gauravhoskote/Notebooks/blob/main/simplenn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msA31qtsCAvU"
      },
      "source": [
        "This is a simple implementation of a neural net."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StLK-2wVTqYU"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = datasets.CIFAR10(\"cifar-10-batches-py\", train = True, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
        "test = datasets.CIFAR10(\"cifar-10-batches-py\", train = False, download = True, transform = transforms.Compose([transforms.ToTensor()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjxJZd5gNJHA",
        "outputId": "cfdf7bff-3fbc-448e-c32d-b69b8f74338b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torch.utils.data.DataLoader(train, batch_size= 10, shuffle = True)\n",
        "testset = torch.utils.data.DataLoader(test, batch_size= 10, shuffle = True)"
      ],
      "metadata": {
        "id": "YvOneB5aNXFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in trainset:\n",
        "  print(data)\n",
        "  break;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjwU5fm1m1Zp",
        "outputId": "05f82608-31b1-4169-c393-a81a64f58753"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[[[0.8588, 0.8510, 0.8510,  ..., 0.8314, 0.8235, 0.8235],\n",
            "          [0.8588, 0.8549, 0.8588,  ..., 0.8275, 0.8157, 0.8196],\n",
            "          [0.8588, 0.8549, 0.8549,  ..., 0.8235, 0.8196, 0.8157],\n",
            "          ...,\n",
            "          [0.6314, 0.3765, 0.2627,  ..., 0.2549, 0.2745, 0.4235],\n",
            "          [0.5294, 0.4314, 0.3216,  ..., 0.4000, 0.3843, 0.4471],\n",
            "          [0.5176, 0.4824, 0.4157,  ..., 0.4745, 0.4510, 0.4941]],\n",
            "\n",
            "         [[0.8588, 0.8510, 0.8510,  ..., 0.8353, 0.8275, 0.8275],\n",
            "          [0.8588, 0.8549, 0.8588,  ..., 0.8353, 0.8196, 0.8235],\n",
            "          [0.8588, 0.8549, 0.8549,  ..., 0.8275, 0.8235, 0.8196],\n",
            "          ...,\n",
            "          [0.7490, 0.4941, 0.4000,  ..., 0.4000, 0.4353, 0.5804],\n",
            "          [0.6510, 0.5608, 0.4784,  ..., 0.5647, 0.5608, 0.6039],\n",
            "          [0.6510, 0.6235, 0.5882,  ..., 0.6118, 0.6000, 0.6196]],\n",
            "\n",
            "         [[0.8588, 0.8510, 0.8510,  ..., 0.8431, 0.8353, 0.8353],\n",
            "          [0.8588, 0.8549, 0.8588,  ..., 0.8392, 0.8275, 0.8314],\n",
            "          [0.8588, 0.8549, 0.8549,  ..., 0.8353, 0.8314, 0.8275],\n",
            "          ...,\n",
            "          [0.7804, 0.5255, 0.4353,  ..., 0.4314, 0.4588, 0.6000],\n",
            "          [0.6824, 0.5961, 0.5137,  ..., 0.5765, 0.5686, 0.6196],\n",
            "          [0.6784, 0.6627, 0.6275,  ..., 0.6275, 0.6118, 0.6471]]],\n",
            "\n",
            "\n",
            "        [[[0.1020, 0.1216, 0.3608,  ..., 0.1961, 0.2667, 0.2941],\n",
            "          [0.1373, 0.1765, 0.4000,  ..., 0.2471, 0.3137, 0.3922],\n",
            "          [0.2157, 0.2353, 0.4039,  ..., 0.2667, 0.2902, 0.2980],\n",
            "          ...,\n",
            "          [0.3647, 0.4667, 0.5569,  ..., 0.6941, 0.6706, 0.6863],\n",
            "          [0.5412, 0.5608, 0.5804,  ..., 0.7059, 0.6863, 0.6824],\n",
            "          [0.5882, 0.5765, 0.5686,  ..., 0.6039, 0.5804, 0.6392]],\n",
            "\n",
            "         [[0.0902, 0.1137, 0.3608,  ..., 0.2118, 0.2902, 0.3216],\n",
            "          [0.1255, 0.1686, 0.4078,  ..., 0.2588, 0.3451, 0.4275],\n",
            "          [0.2078, 0.2314, 0.4157,  ..., 0.2745, 0.3255, 0.3373],\n",
            "          ...,\n",
            "          [0.3529, 0.4549, 0.5451,  ..., 0.6588, 0.6745, 0.7020],\n",
            "          [0.5373, 0.5529, 0.5725,  ..., 0.6941, 0.6980, 0.6980],\n",
            "          [0.5843, 0.5725, 0.5647,  ..., 0.6118, 0.5961, 0.6510]],\n",
            "\n",
            "         [[0.0706, 0.0784, 0.2471,  ..., 0.1333, 0.1490, 0.1490],\n",
            "          [0.0980, 0.1137, 0.2549,  ..., 0.1490, 0.1608, 0.2157],\n",
            "          [0.1647, 0.1529, 0.2392,  ..., 0.1294, 0.1059, 0.1020],\n",
            "          ...,\n",
            "          [0.3373, 0.4510, 0.5373,  ..., 0.7059, 0.6824, 0.7059],\n",
            "          [0.5412, 0.5647, 0.5804,  ..., 0.6824, 0.6471, 0.6510],\n",
            "          [0.6039, 0.5922, 0.5843,  ..., 0.5529, 0.5098, 0.5765]]],\n",
            "\n",
            "\n",
            "        [[[0.4275, 0.4510, 0.4784,  ..., 0.5490, 0.5373, 0.4824],\n",
            "          [0.3922, 0.4314, 0.4510,  ..., 0.5059, 0.5059, 0.4902],\n",
            "          [0.3725, 0.3765, 0.3961,  ..., 0.5176, 0.4824, 0.4745],\n",
            "          ...,\n",
            "          [0.3922, 0.3725, 0.3647,  ..., 0.3961, 0.3961, 0.3765],\n",
            "          [0.3647, 0.3176, 0.2941,  ..., 0.3176, 0.3608, 0.3922],\n",
            "          [0.3451, 0.3098, 0.2863,  ..., 0.2902, 0.3647, 0.3765]],\n",
            "\n",
            "         [[0.5255, 0.5216, 0.5529,  ..., 0.6784, 0.6549, 0.6157],\n",
            "          [0.4980, 0.5098, 0.5373,  ..., 0.6471, 0.6235, 0.6118],\n",
            "          [0.4863, 0.4667, 0.4902,  ..., 0.6627, 0.6118, 0.5922],\n",
            "          ...,\n",
            "          [0.4667, 0.4627, 0.4627,  ..., 0.5137, 0.5137, 0.4824],\n",
            "          [0.4235, 0.4039, 0.3922,  ..., 0.4510, 0.4902, 0.5020],\n",
            "          [0.3961, 0.3843, 0.3765,  ..., 0.4235, 0.4980, 0.4980]],\n",
            "\n",
            "         [[0.3686, 0.3882, 0.4039,  ..., 0.4471, 0.4353, 0.3961],\n",
            "          [0.3373, 0.3725, 0.3843,  ..., 0.3804, 0.3804, 0.3843],\n",
            "          [0.3255, 0.3255, 0.3333,  ..., 0.3882, 0.3451, 0.3529],\n",
            "          ...,\n",
            "          [0.3373, 0.3294, 0.3294,  ..., 0.2941, 0.2941, 0.2784],\n",
            "          [0.2941, 0.2667, 0.2510,  ..., 0.2510, 0.2863, 0.3098],\n",
            "          [0.2667, 0.2510, 0.2353,  ..., 0.2353, 0.2980, 0.3059]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0.2078, 0.2314, 0.1882,  ..., 0.4353, 0.4157, 0.3765],\n",
            "          [0.1843, 0.2314, 0.1961,  ..., 0.4392, 0.4235, 0.4039],\n",
            "          [0.1490, 0.1961, 0.1765,  ..., 0.4824, 0.4745, 0.4588],\n",
            "          ...,\n",
            "          [0.1843, 0.2000, 0.2196,  ..., 0.2118, 0.1882, 0.1490],\n",
            "          [0.1804, 0.2000, 0.2314,  ..., 0.1804, 0.1647, 0.1647],\n",
            "          [0.1765, 0.1922, 0.2118,  ..., 0.1804, 0.1686, 0.1765]],\n",
            "\n",
            "         [[0.2431, 0.2431, 0.1882,  ..., 0.3922, 0.3804, 0.3333],\n",
            "          [0.2196, 0.2392, 0.2000,  ..., 0.3882, 0.3804, 0.3608],\n",
            "          [0.1765, 0.2078, 0.1843,  ..., 0.4118, 0.4078, 0.3961],\n",
            "          ...,\n",
            "          [0.1961, 0.2078, 0.2157,  ..., 0.2039, 0.1922, 0.1608],\n",
            "          [0.1843, 0.2078, 0.2353,  ..., 0.1804, 0.1686, 0.1686],\n",
            "          [0.1843, 0.2039, 0.2157,  ..., 0.1882, 0.1765, 0.1843]],\n",
            "\n",
            "         [[0.0549, 0.0980, 0.0706,  ..., 0.3137, 0.3020, 0.2627],\n",
            "          [0.0471, 0.1020, 0.0902,  ..., 0.3176, 0.3020, 0.2824],\n",
            "          [0.0510, 0.0745, 0.0824,  ..., 0.2902, 0.2863, 0.2745],\n",
            "          ...,\n",
            "          [0.0980, 0.0980, 0.0980,  ..., 0.1255, 0.1137, 0.0902],\n",
            "          [0.1020, 0.0941, 0.0941,  ..., 0.1020, 0.1020, 0.1020],\n",
            "          [0.1137, 0.1255, 0.1451,  ..., 0.1137, 0.1059, 0.1137]]],\n",
            "\n",
            "\n",
            "        [[[0.9765, 0.9686, 0.9725,  ..., 0.9725, 0.9725, 0.9725],\n",
            "          [1.0000, 0.9922, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [0.9843, 0.9843, 0.9922,  ..., 0.9922, 0.9922, 0.9922],\n",
            "          ...,\n",
            "          [0.8863, 0.7686, 0.7843,  ..., 0.7255, 0.5804, 0.4471],\n",
            "          [0.9922, 0.9804, 0.9843,  ..., 0.6706, 0.5843, 0.4627],\n",
            "          [0.9765, 0.9686, 0.9725,  ..., 0.7176, 0.6118, 0.6549]],\n",
            "\n",
            "         [[0.9765, 0.9686, 0.9725,  ..., 0.9725, 0.9725, 0.9725],\n",
            "          [1.0000, 0.9922, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [0.9843, 0.9843, 0.9922,  ..., 0.9922, 0.9922, 0.9922],\n",
            "          ...,\n",
            "          [0.8863, 0.7725, 0.7882,  ..., 0.5608, 0.4314, 0.3647],\n",
            "          [0.9922, 0.9804, 0.9843,  ..., 0.5216, 0.4549, 0.3882],\n",
            "          [0.9765, 0.9686, 0.9725,  ..., 0.6039, 0.5176, 0.6039]],\n",
            "\n",
            "         [[0.9765, 0.9686, 0.9725,  ..., 0.9725, 0.9725, 0.9725],\n",
            "          [1.0000, 0.9922, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
            "          [0.9843, 0.9843, 0.9922,  ..., 0.9843, 0.9843, 0.9843],\n",
            "          ...,\n",
            "          [0.8863, 0.7686, 0.7882,  ..., 0.4157, 0.3294, 0.3176],\n",
            "          [0.9922, 0.9804, 0.9843,  ..., 0.4196, 0.3804, 0.3569],\n",
            "          [0.9765, 0.9686, 0.9725,  ..., 0.5333, 0.4627, 0.5804]]],\n",
            "\n",
            "\n",
            "        [[[0.7333, 0.7255, 0.7333,  ..., 0.7490, 0.7490, 0.7490],\n",
            "          [0.7294, 0.7216, 0.7294,  ..., 0.7451, 0.7451, 0.7451],\n",
            "          [0.7333, 0.7255, 0.7333,  ..., 0.7490, 0.7490, 0.7451],\n",
            "          ...,\n",
            "          [0.3922, 0.3843, 0.3843,  ..., 0.4157, 0.4196, 0.4196],\n",
            "          [0.3843, 0.3882, 0.3922,  ..., 0.3882, 0.4000, 0.4039],\n",
            "          [0.3843, 0.3922, 0.3804,  ..., 0.3843, 0.3882, 0.3922]],\n",
            "\n",
            "         [[0.7529, 0.7451, 0.7529,  ..., 0.7529, 0.7529, 0.7529],\n",
            "          [0.7490, 0.7412, 0.7490,  ..., 0.7490, 0.7490, 0.7490],\n",
            "          [0.7529, 0.7451, 0.7529,  ..., 0.7529, 0.7529, 0.7490],\n",
            "          ...,\n",
            "          [0.4235, 0.4118, 0.4078,  ..., 0.4353, 0.4392, 0.4392],\n",
            "          [0.4157, 0.4157, 0.4157,  ..., 0.4078, 0.4196, 0.4235],\n",
            "          [0.4157, 0.4196, 0.4039,  ..., 0.4039, 0.4078, 0.4118]],\n",
            "\n",
            "         [[0.7294, 0.7216, 0.7294,  ..., 0.7333, 0.7333, 0.7333],\n",
            "          [0.7255, 0.7176, 0.7255,  ..., 0.7294, 0.7294, 0.7294],\n",
            "          [0.7294, 0.7216, 0.7294,  ..., 0.7333, 0.7333, 0.7294],\n",
            "          ...,\n",
            "          [0.3725, 0.3804, 0.3922,  ..., 0.4078, 0.4118, 0.4118],\n",
            "          [0.3647, 0.3804, 0.3961,  ..., 0.3804, 0.3922, 0.3961],\n",
            "          [0.3647, 0.3882, 0.3882,  ..., 0.3765, 0.3804, 0.3843]]]]), tensor([8, 1, 3, 1, 3, 8, 9, 2, 9, 8])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = data[0][0], data[1][0]\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePWsAEKzm9Qc",
        "outputId": "8a156622-7b07-4046-bb3f-7c3e42d30a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "% pip install matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "print(x.shape)\n",
        "#plt.imshow(x.view(32,32))\n",
        "#plt.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQjEKpwNovNe",
        "outputId": "c54e635e-f6cb-4e6a-f1b9-f22336e19575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "torch.Size([3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.fc1 = nn.Linear(3*32*32, 128)\n",
        "      self.fc2 = nn.Linear(128, 128)\n",
        "      self.fc3 = nn.Linear(128, 128)\n",
        "      self.fc4 = nn.Linear(128, 10)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = f.relu(self.fc1(x));\n",
        "    x = f.relu(self.fc2(x));\n",
        "    x = f.relu(self.fc3(x));\n",
        "    x = self.fc4(x);\n",
        "    return f.log_softmax(x, dim=1);\n",
        "\n",
        "    \n",
        "net = Net()\n",
        "print(net)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAbdIm4XpsdZ",
        "outputId": "754fcd11-8f96-49e5-d1ea-2c4a67f0c92a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (fc1): Linear(in_features=2352, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=128, bias=True)\n",
            "  (fc4): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand((28,28))\n",
        "x = x.view(1, 28*28)\n",
        "op = net(x)\n",
        "print(op)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BZACgefdiKH",
        "outputId": "d0da774f-d464-41fb-ce1f-5d64b34a2039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-2.2406, -2.3438, -2.2937, -2.2504, -2.3986, -2.3924, -2.2426, -2.3810,\n",
            "         -2.3024, -2.2030]], grad_fn=<LogSoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(net.parameters(), lr = 0.01)\n",
        "EPOCHS= 2\n",
        "for epoch in range(EPOCHS):\n",
        "  for data in trainset:\n",
        "    x,y = data\n",
        "    net.zero_grad()\n",
        "    output = net(x.view(-1,3*32*32))\n",
        "    loss = f.nll_loss(output, y);#use mse for one hot vec and nll for probability distr\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "7bQaT_dItCFA",
        "outputId": "f51cef68-14e5-48cf-c000-60c37528eb11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-9376f6f3d953>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;31m#use mse for one hot vec and nll for probability distr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-6887354e77f2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x3072 and 2352x128)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for data in trainset:\n",
        "    x,y = data\n",
        "    op = net(x.view(-1, 28*28))\n",
        "    for idx,i in enumerate(op):\n",
        "      if torch.argmax(i) == y[idx]:\n",
        "        correct += 1\n",
        "      total += 1\n",
        "print(\"Accuracy : \", round(correct/total, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc4uDzn1wyJ4",
        "outputId": "5866bb11-81eb-4f19-dd91-bdbe34d620b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  0.929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0705CFlLfpbh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}